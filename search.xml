<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Billion-scale Commodity Embedding for E-commerceRecommendation in Alibaba论文阅读</title>
    <url>/2022/02/24/Billion-scale%20Commodity%20Embedding%20for%20E-commerceRecommendation%20in%20Alibaba%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<p>淘宝推荐系统目前遇到的挑战</p>
<ul>
<li>扩展性——数据集规模大</li>
<li>稀疏性——用户交互行为少</li>
<li>冷启动——新上线<code>item</code>无用户历史交互行为</li>
</ul>
<p>推荐一般有两个阶段：<code>matching &amp; ranking</code>。 本文聚焦在<code>matching</code>阶段，其核心任务是基于用户行为计算所有物品的两两相似度。</p>
<p>对于用户交互行为稀疏的<code>item</code>，学习到准确的<code>embedding</code>向量是很难的事情。所以作者利用<code>side information</code>去<code>enhance the embedding procedure</code>。</p>
<p>那么什么是<code>item</code>的<code>side information</code>？可以简单的理解为<code>item</code>的属性，与用户行为无关。例如品牌、类别、价格等等信息。至此我们可以看到，作者认为<code>side information</code>相近的<code>item</code> 在<code>embedding</code>空间也应该相近。</p>
<p>但是，淘宝商品的<code>side information</code>有上百种，其中哪些<code>side information</code>对<code>embedding</code>重要的？作者设计了权重机制，应用在<code>embedding</code>的训练中。</p>
<p>下面我们来看看如何获得<code>item embedding</code>以及如何将<code>side information</code>融入到<code>item embedding</code>中。</p>
<p>作者利用用户行为进行item graph。用户行为指的是用户历史行为序列，作者利用了历史一小时内的用户行为数据进行训练。原因有以下两点：1.空间和时间限制 2.用户兴趣会随时间变化。在构建图之前，需要对数据进行消噪。可以通过以下几方面进行：</p>
<ol>
<li>点击后停留时间小于1s</li>
<li>过滤异常用户。如果一个用户在三个月内点击超过3500次或者购买物品超过1000次，则认为该用户为异常用户，过滤其所有用户</li>
<li>商家更新商品描述信息太快，导致过了一段时间后同一个identifier对应的item变成了完全不一样的东西</li>
</ol>
<p><strong>构图过程：</strong></p>
<p>1.作者构建的是有向图，每条边的权重等于由<script type="math/tex">item_i</script>到<script type="math/tex">item_j</script>转移次数除以整个图的转移次数。</p>
<p>2.只用同一session中的序列数据进行构建</p>
<h4 id="Base-Graph-Embedding"><a href="#Base-Graph-Embedding" class="headerlink" title="Base Graph Embedding"></a>Base Graph Embedding</h4><p>作者首先定义了节点之间的转移概率：</p>
<p><img src="D:\Users\dz.liu\AppData\Roaming\Typora\typora-user-images\image-20200715175115391.png" alt="image-20200715175115391"></p>
<p>然后基于这个转移概率生成随机游走序列。</p>
<p>后面的过程与基于negative sampling的skip-gram一致。</p>
<h4 id="Graph-Embedding-with-Side-Information"><a href="#Graph-Embedding-with-Side-Information" class="headerlink" title="Graph Embedding with Side Information"></a>Graph Embedding with Side Information</h4><p>加入side information主要是减轻冷启动问题。</p>
<p>那么side information是如何加入的呢？通过加入一个embedding层，为item本身以及n个side information训练<script type="math/tex">n + 1</script>个embedding向量，最后进行average pooling作为输入进行训练。</p>
<p>网络结构如下：</p>
<p><img src="D:\Users\dz.liu\AppData\Roaming\Typora\typora-user-images\image-20200715192816436.png" alt="image-20200715192816436"></p>
<h4 id="Enhanced-Graph-Embedding-with-Side-Information"><a href="#Enhanced-Graph-Embedding-with-Side-Information" class="headerlink" title="Enhanced Graph Embedding with Side Information"></a>Enhanced Graph Embedding with Side Information</h4><p>框架与上图一样，只是对每个side information增加了一个分数，最后采用加权池化</p>
<p><img src="D:\Users\dz.liu\AppData\Roaming\Typora\typora-user-images\image-20200715193921791.png" alt="image-20200715193921791"></p>
]]></content>
  </entry>
  <entry>
    <title>Deep Neural Networks for YouTube Recommendations论文阅读</title>
    <url>/2020/11/27/Deep%20Neural%20Networks%20for%20YouTube%20Recommendations%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</url>
    <content><![CDATA[<h2 id="Deep-Neural-Networks-for-YouTube-Recommendations论文阅读"><a href="#Deep-Neural-Networks-for-YouTube-Recommendations论文阅读" class="headerlink" title="Deep Neural Networks for YouTube Recommendations论文阅读"></a>Deep Neural Networks for YouTube Recommendations论文阅读</h2><p>推荐YouTube videos的挑战主要来自三个方面：</p>
<ul>
<li><p>scale</p>
<p>需要将推荐算法应用在极大规模场景下</p>
</li>
<li><p>freshness</p>
<p>每秒都有新视频上传。推荐系统应该也要对这些新视频做出足够的响应</p>
</li>
<li><p>noise</p>
<p>因为稀疏性和一系列无法被观察到的外部因素，历史用户行为很难被预测。很难有刻画用户满意度的ground truth，所以模型是对含有噪音的隐式反馈信息建模。所以算法应该要对训练数据集中的这些特征具有鲁棒性。</p>
</li>
</ul>
<span id="more"></span>
<p>系统架构</p>
<p><img src="01.png" width="600" height="400" alt="系统架构" align=center /></p>
<p>召回层通过CF只提供了粗粒度的个性化结果。用户之间的相似度利用例如观看视频ID、搜索词、人口特征等计算。</p>
<p>排序层是根据丰富的视频及用户信息，计算候选列表中每一个视频的分数，排序后个数最高的视频展现给用户。</p>
<h4 id="召回层"><a href="#召回层" class="headerlink" title="召回层"></a>召回层</h4><p>将视频库中的视频限制到了几百量级。这里训练采用的是基于rank loss的矩阵分解方法。</p>
<ul>
<li><p>Recommendation as classification</p>
<p>将预测问题看成为一个有着极多类别的分类问题，即精确的从百万量级的视频(每个视频看成为一类)中分类出被观看视频所代表的类别，特征基于用户行为U及context，采用softmax分类函数。</p>
<p><img src="02.png"></p>
<p>深度神经网络的任务是学习到user embeddings，作为用户历史和上下文的函数。</p>
<p>YouTube采用了隐式反馈训练模型，具体来说就是将用户完播一个视频看成正样本。YouTube没有用显式反馈来建模的原因是显式反馈数据实在是太稀疏了。</p>
</li>
</ul>
<p><em>如何将这么多类别的分类问题变得有效率？</em></p>
<p>Step1 负采样</p>
<p>Step2 importance weighting来修正采样</p>
<p>在实践中，负样本采样到几千个，将传统的softmax计算过程加速了100倍。一个比较流行的方法是hierarchical softmax，但是youtube并没有得到相应的准确率。在分层softmax中，遍历树中的每个节点都涉及到区分通常不相关的类集，从而使分类问题更加困难和降低性能。</p>
<p>从百万item中计算TOPN个物品，延迟还需要在10毫秒量级。YouTube之前的系统依赖hash方法。现在，因为不需要softmax输出层标准的似然函数，所以打分问题退化为在点积空间搜索最近邻居节点问题。在A/B实验中，最终的结果对最近邻居节点搜索算法也并不敏感。这里为什么可以退化为在点积空间寻找最近邻问题呢？因为softmax函数分母可以看成常数，所以分子中指数的次方项越大，最后的分数越大，也就等同于点积最大。</p>
<h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>一个用户的观看历史可以被观看序列表示，该序列的大小是固定的。然后根据这个序列，利用embedding转换为稠密向量。池化采用平均池化，其余池化如sum、component-wise、max等均没有平均池化效果好，不过在具体场景中采用什么池化也是需要经过试验验证的。embedding参数是与其他参数一起，通过正常的梯度下降算法训练出来的。</p>
<p><img src="03.png"></p>
<h4 id="Heterogeneous-Signals"><a href="#Heterogeneous-Signals" class="headerlink" title="Heterogeneous Signals"></a>Heterogeneous Signals</h4><p>用户的搜索历史同观看历史一样，每一个搜索词被标记为一元词组或者二元词组，然后将每个标记embedding成向量。然后将所有的标记取平均，最终的向量就可以表示用户的搜索历史了。人口统计学特征对新用户很有用。用户的地理信息、设备信息也被embedding，然后concat到一起。一些简单的二值特征比如用户的性别、登录状态以及一些连续特征如年龄等直接被喂入模型。</p>
<p>“样本年龄”特征</p>
<p>YouTube研究人员持续观察到：用户更喜欢新鲜的内容，尽管并不是很相关。机器学习系统是有偏的，因为模型是利用用户历史数据去预测未来行为的。视频流行度的分布是高度不标准的，但是推荐系统推荐的结果是训练窗口中视频被观看的平均似然值。所以，增加了example age这个特征去训练，结果表明可以很好的刻画视频流行度的分布。</p>
<p>在机器学习问题中，不直接对目标建模，而是对目标相关的指标建模，反而可能达到更好的效果。例如：电影推荐可以通过预估打分来完成 ；在这个应用中，可以用观看时长来预估点击；再例如，点击率预估可以通过对停留时间建模实现。</p>
<p>YouTube的训练数据来自YouTube所有的观看视频，而不仅仅是推荐系统推荐的。否则，新视频很难被曝光，推荐系统也容易有偏。对每个用户固定训练样本，让每个用户在loss function中有一样的权重，来防止头部用户在loss中产生主导。</p>
]]></content>
      <categories>
        <category>CTR prediction</category>
      </categories>
      <tags>
        <tag>CTR</tag>
        <tag>Recommendation</tag>
      </tags>
  </entry>
  <entry>
    <title>Python动态引入包</title>
    <url>/2022/01/09/Python%E5%8A%A8%E6%80%81%E5%BC%95%E5%85%A5%E5%8C%85/</url>
    <content><![CDATA[<p>考虑有这样一个场景：你需要根据入参的不同去引入不同层级下的<code>util</code>模块，比如，入参<code>folder=v1</code>，需要引入<code>./v1/util.py</code>，入参folder=<code>v2</code>，需要引入<code>./v2/util.py</code>。<strong>如何利用一行代码就可以实现根据不同的入参来动态引入不同目录下的模块呢？</strong></p>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> importlib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入python第三方包</span></span><br><span class="line">np = importlib.import_module(<span class="string">&quot;numpy&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(np.random.random((<span class="number">2</span>, <span class="number">3</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 引入模块</span></span><br><span class="line">util = importlib.import_module(<span class="string">&quot;./&#123;&#125;/util&quot;</span>.<span class="built_in">format</span>(folder))</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title>Tensorflow实现cosine similarity</title>
    <url>/2022/01/08/Tensorflow%E5%AE%9E%E7%8E%B0cosine%20similarity/</url>
    <content><![CDATA[<blockquote>
<p>一种简单优雅的实现方法</p>
</blockquote>
<h4 id="余弦相似度计算公式"><a href="#余弦相似度计算公式" class="headerlink" title="余弦相似度计算公式"></a>余弦相似度计算公式</h4><p>$ cos(\vec x,\vec y)=\frac{&lt;\vec x,\vec y&gt;}{|\vec x| |\vec y|} $</p>
<h4 id="最直观实现方式"><a href="#最直观实现方式" class="headerlink" title="最直观实现方式"></a>最直观实现方式</h4><p>设<code>user_side_representation</code>与<code>item_side_representation</code>的<code>shape=[None, 4]</code>，其中<code>None</code>表示<code>batch_size</code>，4表示<code>embedding_size</code>。</p>
<p>计算<code>user_side_representation</code>与<code>item_side_representation</code>两者的<code>cosine similarity</code>最直观的实现方式为：<br>1）求<code>user_side_representation</code>与<code>item_side_representation</code>的内积<code>inner_product</code>;<br>2）分别计算<code>user_side_representation</code>与<code>item_side_representation</code>的<code>L2-norm</code>，<code>norm_user</code>与<code>norm_item</code>;<br>3）计算<code>inner_product / (norm_user*norm_item)</code></p>
<span id="more"></span>
<p>以上思路是完全按照余弦相似度计算公式进行的，利用<code>TensorFlow</code>实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_side_representation_expand = tf.expand_dims(user_side_representation, axis=<span class="number">1</span>)</span><br><span class="line">item_side_representation_expand = tf.expand_dims(item_side_representation, axis=<span class="number">2</span>)</span><br><span class="line">inner_product = tf.matmul(user_side_representation_expand, item_side_representation_expand)</span><br><span class="line"></span><br><span class="line">user_side_representation_norm = tf.sqrt(tf.reduce_sum(tf.square(user_side_representation), axis=<span class="number">1</span>))</span><br><span class="line">item_side_representation_norm = tf.sqrt(tf.reduce_sum(tf.square(item_side_representation), axis=<span class="number">1</span>))</span><br><span class="line">denominoator_norm = tf.expand_dims(user_side_representation_norm * item_side_representation_norm, axis=<span class="number">1</span>)</span><br><span class="line">output = tf.squeeze(inner_product, axis=<span class="number">1</span>) / denominoator_norm</span><br></pre></td></tr></table></figure>
<p><strong>但是该实现方式有个致命的问题：在训练过程中output或者loss值会出现为nan的情况</strong>。其原因在与<strong><code>tf.sqrt(x)</code>中<code>x</code>过小，导致<code>sqrt()</code>函数输出为<code>nan</code></strong>。最终会导致模型无法正确训练。</p>
<h4 id="最实用-amp-优雅实现方式"><a href="#最实用-amp-优雅实现方式" class="headerlink" title="最实用&amp;优雅实现方式"></a>最实用&amp;优雅实现方式</h4><p>既然<code>tf.sqrt</code>函数存在输出为<code>nan</code>的问题，那么我们就要避免使用<code>tf.sqrt</code>函数。</p>
<p>再来看下余弦相似度计算公式，其实可以进一步变形：</p>
<p>$ cos(\vec x,\vec y)=\frac{&lt;\vec x,\vec y&gt;}{|\vec x| |\vec y|}=&lt;\vec e_x, \vec e_y&gt; $​</p>
<p>我们直接计算$\vec x$与$\vec y$​两个向量方向上的单位向量的内积就可以了，这样不需要使用<code>tf.sqrt</code>函数。</p>
<p>利用<code>TensorFlow</code>实现如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">user_side_representation_l2norm = tf.nn.l2_normalize(user_side_representation, axis=<span class="number">1</span>)</span><br><span class="line">item_side_representation_l2norm = tf.nn.l2_normalize(item_side_representation, axis=<span class="number">1</span>)</span><br><span class="line">inner_product = tf.reduce_sum(user_side_representation_l2norm * item_side_representation_l2norm, axis=<span class="number">1</span>)</span><br><span class="line">output = tf.clip_by_value(inner_product, clip_value_min=-<span class="number">1.0</span>, clip_value_max=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>利用<code>TensorFlow</code>计算余弦相似度，计算两向量的单位向量的内积即可。</p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>tf.Print()用法</title>
    <url>/2022/01/09/tf.Print()%E7%94%A8%E6%B3%95/</url>
    <content><![CDATA[<p><strong>环境：</strong><code>tensorflow 1.14.0</code></p>
<p>在模型训练的过程中，经常会有实时观察中间<code>tensor</code>值的需求。<code>Python</code>自带的<code>print</code>函数只能打印出<code>tensor</code>或者<code>op</code>的名字、属性信息等等，所以需另辟蹊径。</p>
<span id="more"></span>
<p>实际上，在模型训练时，打印出某些<code>tensor</code>的中间值，有两种实现方法：</p>
<ul>
<li>运行<code>sess.run(tensor)</code>，可以得到<code>tensor</code>的值</li>
<li>采用<code>tf.Print(tensor)</code>函数，直接打印出<code>tensor</code>的当前值</li>
</ul>
<p>第一种方法的典型场景是<strong>间隔性的获取结果类型<code>tensor</code>值</strong>。比如，在训练过程中每迭代100次打印出当前的<code>loss</code>值以便观察。</p>
<p>当然，如果<code>tensor</code>的值不依赖于<code>placeholder</code>的计算，比如某一层的权重<code>tensor-W</code>，调用<code>sess.run(tensor)</code>时是不需要指定<code>feed_dict</code>的。这样也可以实现在训练过程中打印出<code>tensor</code>值的目标。</p>
<p>第二种方法<strong>更加具有普适性，可以打印出任意<code>tensor</code>在当前迭代时的值</strong>。不过，在使用<code>tf.Print()</code>函数时，需要用静态图的视角去考虑。</p>
<p><code>tf.Print()</code>其实是静态图中的一个<code>op</code>节点。如果在运行一个op时，数据流没有流经该<code>tf.Print op</code>节点，则<code>tf.Print()</code>不会生效。如果加入了<code>tf.Print</code>却没有按预期打印出想要的结果，建议仔细检查下静态图逻辑。</p>
<p>数据流入<code>tf.Print</code>节点后，会原样返回。所以<code>tf.Print</code>可以看做恒等输出<code>op</code>，只是打印出了流入数据的值而已。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.Print(</span><br><span class="line">    input_,</span><br><span class="line">    data,</span><br><span class="line">    message=<span class="literal">None</span>,</span><br><span class="line">    first_n=<span class="literal">None</span>,</span><br><span class="line">    summarize=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>input_</code>为输入的<code>tensor</code>，会原样返回；<br><code>data</code>为需要打印的<code>tensor</code>；<br><code>message</code>为日志输出时的前缀信息；</p>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Debug</tag>
      </tags>
  </entry>
  <entry>
    <title>tf.keras如何解决load model时custom loss function的各种报错</title>
    <url>/2021/12/13/tf.keras%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3load%20model%E6%97%B6custom%20loss%20function%E7%9A%84%E5%90%84%E7%A7%8D%E6%8A%A5%E9%94%99/</url>
    <content><![CDATA[<blockquote>
<p>本文适用于以下场景（假定<code>model</code>中含有复杂的<code>custom loss function</code>）：</p>
<ul>
<li><code>load</code>已训练完成的<code>model</code>，进行<code>infer</code></li>
<li>模型滚动更新：加载旧模型，在旧模型的基础上进行增量训练得到新模型</li>
</ul>
</blockquote>
<h4 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h4><ul>
<li><code>Tensorflow2.0.0</code></li>
</ul>
<span id="more"></span>
<h4 id="定义custom-loss-function"><a href="#定义custom-loss-function" class="headerlink" title="定义custom loss function"></a>定义<code>custom loss function</code></h4><ul>
<li><p>这里定义的是<code>pairwise</code>场景下<code>binary crossentropy</code>的<code>loss function</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pairwise_binary_crossentropy</span>(<span class="params">query</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    pairwise_binary_crossentropy, each pair example uses binary crossentropy as loss.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        query: query id</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">y_true, y_pred</span>):</span></span><br><span class="line">        pair_mask = tf.equal(query, tf.transpose(query))</span><br><span class="line">        pair_mask = tf.cast(pair_mask, tf.float32)</span><br><span class="line">  </span><br><span class="line">        <span class="comment">#根据pair_mask创建对角矩阵。 </span></span><br><span class="line">        pair_mask_diag = tf.linalg.band_part(tf.ones_like(pair_mask), -<span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">        pair_mask_diag = tf.linalg.band_part(pair_mask_diag, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">        pair_mask_diag_reverse = <span class="number">1</span> - pair_mask_diag</span><br><span class="line">  </span><br><span class="line">        pair_mask = pair_mask * pair_mask_diag_reverse</span><br><span class="line">  </span><br><span class="line">        si_minus_sj = y_pred - tf.transpose(y_pred)</span><br><span class="line">  </span><br><span class="line">        yi_minus_yj = y_true - tf.transpose(y_true)</span><br><span class="line">        yi_minus_yj = tf.maximum(tf.minimum(<span class="number">1.</span>, yi_minus_yj), -<span class="number">1.</span>)</span><br><span class="line">  </span><br><span class="line">        yi_minus_yj = <span class="number">0.5</span> * (<span class="number">1</span> + yi_minus_yj)</span><br><span class="line">  </span><br><span class="line">        logloss = tf.nn.sigmoid_cross_entropy_with_logits(labels=yi_minus_yj, logits=si_minus_sj)</span><br><span class="line">  </span><br><span class="line">        num_pairs = tf.reduce_sum(pair_mask)</span><br><span class="line">  </span><br><span class="line">        loss = pair_mask * logloss</span><br><span class="line">        loss = tf.reduce_sum(loss)</span><br><span class="line">        res = loss / (num_pairs + <span class="number">0.00001</span>)</span><br><span class="line">  </span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
<p>可以看出，这里不仅仅是<code>loss_function(y_true, y_pred)</code>形式了，而是最外层还有<code>query</code>这个输入。</p>
</li>
</ul>
<h4 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h4><ul>
<li><p><strong>对于利用<code>tf.keras.models.load_model()</code>加载模型，然后直接输入测试数据进行<code>infer</code>的场景</strong></p>
<p>此时若不调用<code>model.compile()</code>进行编译的话，是会报错的，无法得到<code>prediction</code>。而将<code>custom loss function</code>传入<code>model.compile(loss=)</code>中<code>loss</code>参数的话，报的错更是千奇百怪。<strong>我们只需要给<code>loss</code>传入任意一个 keras 提供的 <code>loss function</code>，如 <code>binary_crossentropy</code> 即可。如<code>model.compile(loss=tensorflow.keras.losses.binary_crossentropy)</code>。</strong> </p>
<p>因为 infer 时就是 network 的前向传播计算过程，与 loss 的具体形式无关，只与网络结构与权重有关。所以，即使随机指定 loss 形式，只要<code>model.compile()</code>通过即可。经实测，不同的<code>loss function</code> 形式确实不会影响最终的<code>prediction</code>结果。不放心的小伙伴可以自己尝试。</p>
</li>
<li><p><strong>对于模型滚动更新场景</strong></p>
<p>按常理说，模型滚动更新的步骤应该是这样的：1.加载模型，得到基准模型对象<code>model</code>；2.读取训练数据，利用基准模型对象，调用<code>model.fit()</code>接口，进行模型增量训练。</p>
<p>理想很美好，现实很骨感。上述做法中，<strong>必须先调用<code>model.compile()</code>指定<code>loss</code>以及<code>optimizer</code>后，才可以正常的调用<code>model.fit()</code>，执行后续的训练流程。</strong></p>
<p>好，现在又绕回来了。只要调用<code>model.compile()</code>，就逃不过<code>model.compile(loss=xxx)</code>中<code>custom loss function</code>如何正确传入的问题。<strong>这里与infer场景不同，是无法随意传入<code>loss function</code>让其compile通过即可，而是一定要指定自定义的<code>custom loss function</code>，因为模型滚动更新时训练需要利用自定义的<code>custom loss function</code>训练更新权重。</strong></p>
<p>「Restore 模型后得到基准模型对象，并在此基础上调用fit接口进行训练」这条路已经走到死胡同了，那我们不妨退出来再看看有什么可以“曲线救国”的方法。</p>
<p>明确一点：模型更新的实质是什么？模型更新的实质是<strong>参数的更新</strong>。那么模型滚动更新的实质是什么？其实质是<strong>在现有模型的参数基础上增量更新</strong>。</p>
<p>所以，我们只要从0构建与基准模型拥有完全相同结构的网络，并将基准模型的权重赋值给新构建的网络，再调用fit接口就可以实现模型增量更新的功能。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title>一文带你吃透AUC</title>
    <url>/2022/02/17/%E4%B8%80%E6%96%87%E5%B8%A6%E4%BD%A0%E5%90%83%E9%80%8FAUC/</url>
    <content><![CDATA[<blockquote>
<p>本文旨在从所有教科书都讲到的AUC基本概念为起点，逐步带领大家进入AUC背后更为广阔、神奇的世界。</p>
</blockquote>
<h4 id="一、什么是AUC"><a href="#一、什么是AUC" class="headerlink" title="一、什么是AUC"></a>一、什么是AUC</h4><p><code>AUC</code>的全称为<code>Area Under the Curve</code>，即曲线下的面积。这里的曲线指的是什么曲线？有两种类型：<strong><code>ROC</code>曲线</strong>和<strong><code>PR</code>曲线</strong>。所以，<code>AUC</code>也会有两种类型：<code>ROCAUC</code>及 <code>PRAUC</code>，分别对应ROC曲线下的面积以及PR曲线的的面积。</p>
<span id="more"></span>
<h4 id="二、ROC曲线及PR曲线"><a href="#二、ROC曲线及PR曲线" class="headerlink" title="二、ROC曲线及PR曲线"></a>二、ROC曲线及PR曲线</h4><p>​    在二分类问题中，模型输出得分$S$后（这里的得分指的是<strong>属于正例的概率值</strong>，即经过sigmoid函数归一化到<code>[0,1]</code>内的分数），需要设置一个阈值$T$来决定属于正例还是负例。若$S&gt;T$被判为正例，反之则被判为负例。</p>
<p>​    最终的预测结果，肯定会出现分类错误的情况。根据样本label以及最终模型预测的结果，可以将最终分类结果分为4种情况：</p>
<p><strong>a. label为正，模型预测为正： 真正例 TP(True Positive)</strong><br><strong>b. label为正，模型预测为负： 假负例 FN(False Negative)</strong><br><strong>c. label为负，模型预测为正： 假正例 FP(False Positive)</strong><br><strong>d. label为负，模型预测为负： 真负例 TN(True Negative)</strong></p>
<p>​    按照以下规则会便于分辨与记忆：<strong>正负例指模型最终预测结果，真假指模型是否预测正确。</strong> 所以，举个例子，当计算数据集中正样本的个数为多少时，只需要计算<strong>模型预测为正且预测正确的样本个数+模型预测为负且模型预测错误的样本个数</strong>，按照上述的记忆方法，即$TP+FN$。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center"><strong>Label=1</strong></th>
<th style="text-align:center"><strong>Label=0</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><strong>Pred=1</strong></td>
<td style="text-align:center">TP</td>
<td style="text-align:center">FP</td>
</tr>
<tr>
<td style="text-align:center"><strong>Pred=0</strong></td>
<td style="text-align:center">FN</td>
</tr>
</tbody>
</table>
</div>
<p>​    请大家牢记住上述四种情况，ROC曲线和PR曲线与其息息相关。</p>
<p>​    <strong>ROC和PR曲线到底与$TP、FP、FN、TN$有什么关系呢？</strong> ROC曲线和PR曲线的差别就在于<strong>两者的横纵坐标轴的含义不一样。</strong> ROC曲线的X轴为<code>FPR(False Positive Rate)</code>，Y轴为<code>TPR(True Positive Rate)</code>。而PR曲线的X轴为<code>Recall</code>，Y轴为<code>Precision</code>。</p>
<blockquote>
<p>FPR: 假正例比例。问题是假正例与什么的比例？因为假正例其实是真实的负例的一部分，所以，这里应该是假正例与所有真实负例的比值。于是得到：$FPR=\frac{FP}{TN+FP}$</p>
<p>TPR: 真正例比例。这里是真正例与所有真实正例的比值。于是得到：$TPR=\frac{TP}{TP+FN}$</p>
<p>Recall: 召回率。<strong>召回指的是模型预测的真正例占所有正例的比值</strong>，即$Recall=\frac{TP}{TP+FN}$</p>
<p>Precision: 准确率。<strong>准确率指的是模型预测的真正例占模型预测所有正例的比值</strong>，即$Precision=\frac{TP}{TP+FP}$</p>
<p><strong>总结：</strong> 召回率和准确率可以通俗的来理解：召回率是模型可以从所有的正例中准确的捞出正例样本的能力，准确率是描述模型准确预测正例的能力。从公式可以看出，真正例比例与召回率的形式完全一样，即$TPR=Recall$。而关于FPR，1-FPR的含义其实就是负例样本的召回率，这点可以想想为什么吗？</p>
</blockquote>
<p>&ensp;  ROC曲线及PR曲线的横纵坐标定义好后，曲线的绘制过程一致：将判断正负例的阈值$T$由模型预测得分最小值$S_{min}$至模型预测得分最大值$S_{max}$进行遍历。对于每个时刻的阈值$T_{i}$，计算X轴及Y轴对应的数值，作为这一时刻的坐标。当遍历过程完成后，就会得到ROC曲线或者PR曲线了。</p>
<p><img src="01.png" width="600" height="400" alt="系统架构" align=center /></p>
<p><img src="02.png" width="600" height="400" alt="系统架构" align=center /></p>
<h4 id="三、ROC曲线与PR曲线的对比"><a href="#三、ROC曲线与PR曲线的对比" class="headerlink" title="三、ROC曲线与PR曲线的对比"></a>三、ROC曲线与PR曲线的对比</h4><p>​    从两图中的对比可以很明显的看出，ROC曲线朝左上角“凸起”，而PR曲线是朝右上角“凸起”。如何记忆这一特征呢？<strong>只需将判断正负例的阈值$T$分别置为模型预测得分最小值$S_{min}$及模型预测得分最大值$S_{max}$，计算对应的TPR、FPR及Recall、Precision即可。</strong></p>
<p>​    当$T=S_{min}$，所有样本均被判为正例，于是有：</p>
<p>​        $FPR = \frac{FP}{TN+FP}=\frac{FP}{0+FP}=1$</p>
<p>​        $TPR = \frac{TP}{TP+FN}=\frac{TP}{TP+0}=1$</p>
<p>​        $Recall = \frac{TP}{TP+FN}=TPR=1$</p>
<p>​        $Precision = \frac{TP}{TP+FP}$</p>
<p>​    当$T=S_{max}$，所有样本均被判为负例，于是有：<br>​        $FPR = \frac{FP}{TN+FP}=\frac{0}{TN+0}=0$</p>
<p>​        $TPR = \frac{TP}{TP+FN}=\frac{0}{0+FN}=0$</p>
<p>​        $Recall = \frac{TP}{TP+FN}=TPR=0$</p>
<p>​        $Precision = \frac{TP}{TP+FP}=\frac{0}{0}$</p>
<p>根据上述计算过程，可以看到ROC曲线的两个极值点坐标为<code>(1,1)</code>及<code>(0,0)</code>。当所有样本完全分类正确时，$FPR=\frac{FP}{TN+FP}=\frac{0}{TN}=0$，$TPR = \frac{TP}{TP+FN}=\frac{TP}{TP+0}=1$，即模型完美分类样本时，对应ROC曲线的点在<code>(0,1)</code>。但是模型总会有误差，所以不可能达到<code>(0,1)</code>这个点，只能尽力向<code>(0,1)</code>点靠拢。通过这样简单的分析有助于帮你记忆<strong>ROC曲线越往左上方靠分类效果越好</strong>这个性质。</p>
<p>而PR曲线就要我们好好来研究一下了。PR的两个极值点为A(1, $\frac{TP}{TP+FP}$ (样本全判正))及B(0, $\frac{0}{0}$(样本全判负))。对于A点，当样本全判正时，$\frac{TP}{TP+FP}$其实表示<strong>数据集中真实正样本的比例$p$。</strong> 所以，A点可以表示为(1,p)。对于B点，当样本全判负时，Precision会面临一个问题：<strong>除0</strong>——此时TP及FP均为0。这在数学上是没有意义的，所以通常<strong>利用上限1表示PR曲线中Recall=0时的Precision。</strong> 上限为1又是如何推算出来的呢？个人理解，考虑这样一个场景，当模型判正阈值$T$很高，高到只有一个样本被判为正例，且该正例是真正例，此时的$Precision=\frac{TP}{TP+FP}=\frac{1}{1+0}=1$。当模型完美预测所有样本的结果时，这时的Precision及Recall均为1，所以<strong>PR曲线越往右上角凸起分类结果越好</strong>。这样，大家便可以直观上理解ROC曲线以及PR曲线了。</p>
<p>PR曲线以及ROC曲线之间到底有什么关系呢？推荐大家看一篇论文《 The Relationship Between Precision-Recall and ROC Curves》。这篇论文深入的讨论了PR曲线与ROC曲线之间的爱恨情仇，这里就略微总结一下：</p>
<ul>
<li>对于一个给定正负样本的数据集，PR曲线与ROC曲线的点是一一对应的；</li>
<li>在正负样本不均衡的场景下，PR曲线更能反应分类的性能；</li>
<li>当且仅当曲线A比曲线B在PR空间中更占优势时，曲线A比曲线B才会在ROC空间更有优势；</li>
</ul>
<p>具体的证明过程暂且不表，感兴趣的同学可以阅读下原论文。也可以直接记住结论。</p>
<h4 id="三、AUC评价指标"><a href="#三、AUC评价指标" class="headerlink" title="三、AUC评价指标"></a>三、AUC评价指标</h4><p>原始AUC值的适用场景为二分类场景。一般在各种文章或者日常交流中，大家都会经常听到离线指标<code>AUC</code>是多少多少，这里的AUC普遍指<code>ROCAUC</code>指标。所以，我们先从<code>ROCAUC</code>谈起。上文说过，<code>ROC</code>曲线越向左上角偏分类结果越好，即<code>ROC</code>曲线的<code>AUC</code>值越大越好。当<code>AUC=0.5</code>时，分类效果等于随机猜测，<code>AUC</code>越接近于1，分类效果越好。不过，如果<code>AUC</code>值过大的话，要十分警惕模型发生过拟合。</p>
<p>以上，都是从理论层面在讨论ROC曲线。现在考虑这样一个问题：给定一个数据集，包括真实label以及模型预测结果，我们对整个数据集进行随机均匀负采样，负采样前后的AUC会变化吗？答案是不会。<strong>其实ROCAUC值是有实际意义的：根据真实label随机取正负样本对，模型对正例的预测分数比对负例的预测分数高的概率。</strong> 举个例子吧，假设有以下数据集：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"><strong>样本id </strong></th>
<th style="text-align:center"><strong>真实label </strong></th>
<th style="text-align:center"><strong>模型分数</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">A</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.2</td>
</tr>
<tr>
<td style="text-align:center"></td>
<td style="text-align:center">0</td>
<td style="text-align:center">0.8</td>
</tr>
<tr>
<td style="text-align:center">C</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0.5</td>
</tr>
<tr>
<td style="text-align:center">D</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0.6</td>
</tr>
</tbody>
</table>
</div>
<p>那么所有的正负样本对可以列出来：<code>(A,C)</code>,<code>(A,D)</code>,<code>(B,C)</code>,<code>(B,D)</code>。其中，<code>(A,C)</code>,<code>(A,D)</code>两对中正例的模型分比负例的模型分高。所以，该数据集的AUC值为 <code>2/4=0.5</code>。回过头来看上面这个问题，假设数据集中有N个正样本，有M个负样本，故正负样本对的个数为NM。不妨假设其中有x对正样本模型分是比负样本模型分大的。所以，负采样前的原始<code>AUC</code>分为$\frac{x}{NM}$。好，下面我们以<code>r(r&lt;1)</code>的比例进行负采样，就是将负样本由M个采样至rM个。正负样本对的个数变成了$N<em>r</em>M$。下面的这点对该问题的理解非常重要：<strong>因为采样是均匀的，所以对于某个正样本，其对应的模型分为x，模型分大于x的负例及模型分小于x的负例也应该是以r的比例被采样的。这就导致采样后正样本模型分比负样本模型分大的正负样本对也同时由r变为rx。</strong> 采样后的AUC值为$\frac{rx}{r*NM}=\frac{x}{NM}$。可以看到采样前后的AUC值基本是不变的。</p>
<p>从另一个角度来说，<strong>负采样后的AUC是原始数据集的无偏估计</strong>。</p>
<p>现在给定一个数据集，包含label以及模型预测分，如何计算ROCAUC？</p>
<ol>
<li><p>遍历所有正负样本pair，假定有M个。统计其中正样本模型分&gt;负样本模型分的pair数，假定有N个。那么最终的AUC可以通过$\frac{N}{M}$计算得出。</p>
</li>
<li><p>利用公式求解。原理与1完全一样，只是用公式表达出来。乍一看可能会有点复杂，但是经过拆解后就会发现其实很简单：</p>
</li>
</ol>
<p>假设有M个正样本，N个负样本，将样本根据模型分大小从小到大排列。</p>
<p>$AUC=\frac{\sum_{i=1}^{M}(RANK(p_i)-i)}{MN}$，$p_i$表示第i个正样本，$RANK(p_i)$表示第i个正样本的排名。</p>
<p>提示：举个简短的例子代入到公式中会更加直观。</p>
<h4 id="四、gAUC值"><a href="#四、gAUC值" class="headerlink" title="四、gAUC值"></a>四、gAUC值</h4><p><code>gAUC=group AUC</code>。这个group一词就表达了gAUC的精华所在。AUC表示是所有样本间的相互比较。而gAUC引入了组（group）的概念，组内相互比较，最后再通过组加权的方式算出最终的gAUC值。</p>
<p>那么为什么要引入group的概念呢？思考下面这个场景就明白了：在搜索排序场景，一个query对应一个排序列表。数据集中肯定不止一个query，那么不同query之间模型预测分数的值之间相互比较有意义吗？没有意义！</p>
<p>搜三亚出来的产品A的得分与搜广州出来的产品B的得分当然没有可比性。但是搜三亚出来的产品A的得分与搜三亚出来的产品B的得分这就有比较意义了。</p>
<p>所以gAUC首先计算各group内的AUC，最终根据各group的权重进行加权求和。公式如下：</p>
<p>$gAUC=\frac{\sum_{i\in N}{w_i * AUC_i}}{\sum_{i \in N }{w_i}}$</p>
<p>这里的group权重可以根据不同的策略制定。</p>
<h4 id="结尾"><a href="#结尾" class="headerlink" title="结尾"></a>结尾</h4><p>个人水平有限，小伙伴有什么补充或者好的建议，欢迎评论，一起探讨！</p>
]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Metrics</category>
      </categories>
      <tags>
        <tag>Metrics</tag>
      </tags>
  </entry>
  <entry>
    <title>谈谈id类特征过拟合问题</title>
    <url>/2022/04/04/%E8%B0%88%E8%B0%88id%E7%B1%BB%E7%89%B9%E5%BE%81%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98/</url>
    <content><![CDATA[<h4 id="一、背景"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景</h4><p>组内最近准备上线<code>ESMM</code>模型，其中加入了用户的点击行为序列与候选物品的相似度特征。具体的做法是：根据用户点击过的<code>item</code>序列，经过<code>Avg Pooling</code>计算出<code>representation vector</code>。而后将其与<code>candidate item</code>的<code>embedding vector</code>做<code>inner product</code>，计算用户行为序列与当前候选<code>item</code>的相似度。</p>
<p>离线训练时发现，在第一个<code>epoch</code>中，<code>train loss</code>与<code>val loss</code>在同步下降。而在第二个<code>epoch</code>开始后，<code>train loss</code>下降，<code>val loss</code>却在一直上升。很明显，模型过拟合了。</p>
<p>尝试将该特征剔除后，模型恢复到需要多个<code>epoch</code>才能收敛的状态。由此我们确定<strong>由于加入了id类信息，导致了模型的过拟合</strong>。</p>
<p>类似的，阿里的<code>DIN</code>原论文中也提到了这个现象：</p>
<blockquote>
<p><code>Overfitting is a critical challenge for training industrial networks.For example, with addition of fine-grained features, such as features of goods_ids with dimensionality of 0.6 billion (including visited_goods_ids of user and goods_id of ad as described in Table 1), model performance falls rapidly after the first epoch during
training without regularization.</code></p>
</blockquote>
<h4 id="二、原因"><a href="#二、原因" class="headerlink" title="二、原因"></a>二、原因</h4><p>我们认为，出现过拟合的原因在于：<code>id</code>类特征取值过大，各个<code>id</code>在训练集中出现的次数并不多，也就是说大部分<code>id</code>在训练集中都是相对低频的。而低频就导致模型训练时容易记录这些出现过的<code>pattern</code>，这些<code>id</code>特征的<code>representation</code>在训练时尽可能倾向于表达这些出现过的<code>pattrn</code>，而对测试集中未出现过的<code>pattern</code>表达效果却不佳，这就造成了过拟合的现象。</p>
<p>这里的<code>pattern</code>是什么意思？举个例子，用户A之前的点击<code>id</code>序列为<code>[101,302,463]</code>，当前点击<code>item id</code>为<code>965</code>，那么这里的<code>pattern</code>可以理解为<code>101、302、463</code>这些<code>item</code>比物料库中其它<code>item</code>的<code>representation</code>更接近于<code>item_id=965</code>。模型会学到这个信息。</p>
<p>举个极端点的例子，上述的<code>pattern</code>只出现过一次，也就是说<code>101、302、463</code>在训练集中只出现过一次，同时其对应的<code>click item_id=965</code>也只出现过一次，那么模型就会非常强烈的记住这个信息，以至于在测试集中就算某个用户点击<code>id</code>序列为<code>[101,302,463]</code>时，<code>candidate item_id</code>不等于<code>965</code>，效果就有可能变得很差。</p>
<p>尤其是用户交互行为稀疏的场景，该现象会更明显。用户行为稀疏的含义包含两方面：1. 有交互行为的用户占比少；2. 用户交互行为长度短，比如有行为用户的平均交互长度为1.x。</p>
<p>缓解过拟合问题，有两种解决方法：</p>
<ul>
<li>丰富用户行为</li>
<li>参数正则</li>
</ul>
<p>第一种方法，几乎很难实现，要求场景的流量大幅提升或者采取某些策略可以强烈吸引用户点击的欲望。<code>Rome wasn&#39;t built in a day.</code>并且，如果物料库太大，如阿里的商品<code>id</code>到达<code>0.6 billion</code>级别，过拟合的现象不可避免的还是会发生。</p>
<p>所以推荐第二种方法，对<code>id embedding</code>向量进行正则。这又引入了一个问题，常规的正则需要对存储<code>id embedding</code>词表的所有参数进行正则计算。这种计算量是不可接受的。仔细想想，每一个<code>mini batch</code>更新时，其实只有在该<code>batch</code>中出现过的<code>id embedding vector</code>才会进行更新，那我们只对这部分参数进行正则计算就可以解决计算复杂度的问题。阿里的<code>DIN</code>论文就是这么做的，有兴趣的同学可以看看。</p>
<h4 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h4><p>一般来说，<code>id</code>类特征是强特征，用好它们会给模型带来比较好的收益，但是一定要注意由此引入的过拟合问题，可以通过正则解决。</p>
<h4 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h4><ol>
<li>阿里<code>DIN</code>论文: <em>‘Deep Interest Network for Click-Through Rate Prediction’</em></li>
</ol>
]]></content>
      <categories>
        <category>CTR</category>
      </categories>
      <tags>
        <tag>原理思考</tag>
      </tags>
  </entry>
</search>
